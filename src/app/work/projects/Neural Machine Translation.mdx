---
title: "Neural Machine Translation"
publishedAt: "2024-10-11"
summary: "Implemented and benchmarked a full pipeline for Neural Machine Translation (NMT) using progressively advanced architectures — from vanilla RNNs and LSTMs to attention-based Seq2Seq and finally Transformer models — trained and tuned from scratch in PyTorch. Evaluated models on translation perplexity, stability, and generalization, revealing how attention and parallel self-attention mechanisms revolutionize sequence modeling."
images:
  - "/images/projects/project-01/cover-04.jpg"
  - "/images/projects/project-01/video-01.mp4"
---


team:
  - name: "Ketu Patel"
    role: "AI/ML Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/ketu-patel/"
---


## Overview

The project explored how machines learn to translate human language using deep sequence models. Beginning with recurrent architectures (RNNs, LSTMs), it progressed to attention-augmented Seq2Seq systems and ultimately to the Transformer, analyzing how architectural evolution impacts gradient flow, alignment, and translation coherence.

All models were trained on paired English captions and corresponding target translations using PyTorch implementations built from scratch, without relying on high-level libraries. Metrics such as loss and perplexity were tracked across training and validation to quantify language-model confidence and generalization.

### Table of Contents


## **1. Key Features**

This section breaks down the five major model architectures implemented — each representing a distinct stage in the evolution of deep sequence modeling for translation.

### **1.1 Variational Autoencoder (VAE)**

- The baseline model implemented a classical sequence-to-sequence (Seq2Seq) structure with separate encoder and decoder RNNs.
- Each encoder step processes one token at a time and updates a hidden state, which serves as a running summary of all previous tokens. After reading the entire input sequence, the final hidden state of the encoder is passed to the decoder as a condensed vector representation of the source sentence. The decoder then generates each target token autoregressively.
- INSERT IMAGE HERE
- This simple recurrent formulation forces the entire source sentence into a single vector representation. While computationally light, this approach suffers from the vanishing gradient problem, causing earlier words to be forgotten as new tokens are processed. This limitation makes it difficult for the model to retain long-range dependencies — particularly in long or complex sentences.
- In the translation pipeline, the encoder–decoder architecture was identical to the RNN model, but with LSTM cells enabling better temporal context and richer sequence representations. The LSTM provided the foundation for robust and stable optimization in the later attention and transformer experiments.

---

### **1.2 Long Short-Term Memory (LSTM)**

- To overcome the gradient vanishing problem of vanilla RNNs, the next model replaced the recurrent cell with a Long Short-Term Memory (LSTM) unit.
- The LSTM introduces three gating mechanisms — input, forget, and output gates — that regulate information flow and maintain a persistent cell state across time steps:
- INSERT IMAGE HERE
- This gating mechanism acts like a controlled memory cell, preserving long-term dependencies and selectively forgetting irrelevant information. Unlike an RNN’s compressed hidden state, the LSTM’s dual-state structure allows gradient flow to remain nearly constant through time, dramatically improving convergence stability.
- 

----

### **1.3 Diffusion Models (DDPM)**

- Developed an end-to-end **Denoising Diffusion Probabilistic Model** following *Ho et al., 2020*.
  - **Forward Process:** Gradually corrupt images with Gaussian noise:
    <Image src="/images/projects/Image_Generation/Forward_diffusion.png" alt="Forward Diffusion" title="Forward Diffusion Formula" size="40" />
    <Image src="/images/projects/Image_Generation/Forward_diff_epoch.png" alt="Forward Demo" title="Forward Diffusion Epoch" size="95" />


  - **Reverse Process:** Train a neural network `ε_θ(x_t, t)` (UNet) to predict noise at each timestep and iteratively denoise to reconstruct `x_0`.
    <Image src="/images/projects/Image_Generation/Reverse_diffusion.png" alt="Reverse Diffusion" title="Reverse Diffusion Formula" size="40" />
    <Image src="/images/projects/Image_Generation/Reverse_diff_epoch.png" alt="Reverse Demo" title="Reverse Diffusion Epoch" size="95" />

  - Implemented a **linear noise schedule** and cached α_t and ᾱ_t for training efficiency.

- Visualized both **forward diffusion** (progressive noising) and **reverse diffusion** (iterative denoising) to interpret model behavior.

----

## **2. Technologies / Algorithms Used**
--- 

<TechTable leftHeader="Category" rightHeader="Details" alignDetails="left">
  <TechRow label="Frameworks">
    PyTorch 2.x, TorchVision, NumPy, Matplotlib, PyYAML
  </TechRow>

  <TechRow label="Datasets">
    MNIST and Fashion-MNIST (28×28 grayscale, 10 classes)
  </TechRow>

  <TechRow label="Architectures">
    VAE (Encoder–Decoder MLP), GAN (Generator + Discriminator), DDPM (UNet)
  </TechRow>

  <TechRow label="Optimization">
    AdamW optimizer, weight decay tuning, learning-rate scheduling
  </TechRow>

  <TechRow label="Loss Functions">
    L1, L2, KL-Divergence, Binary Cross-Entropy
  </TechRow>

  <TechRow label="Noise Scheduling (Diffusion)">
    Linear variance schedule βₜ ∈ [0.0001, 0.02]
  </TechRow>

  <TechRow label="Evaluation Metric">
    Frechet Inception Distance (FID) for realism and diversity
  </TechRow>

  <TechRow label="Config System">
    YAML-driven experiment configs with tracked outputs
  </TechRow>

  <TechRow label="Environment">
    Python 3.11, GPU-accelerated PyTorch, Dockerized setup
  </TechRow>

  <TechRow label="Optimization">
    AdamW optimizer, weight decay tuning, learning-rate scheduling
  </TechRow>

</TechTable>

---

## **3. Challenges & Learning**

### 3.1 Balancing Reconstruction vs Regularization (VAEs)
- High β values improved disentanglement but caused **posterior collapse**.
- Low β favored reconstruction detail but weakened latent structure.
- Learned to **tune β adaptively** by dataset complexity.

#### 3.1.1 Loss Dynamics (L1 vs L2)
- L2 minimized squared error → smoother but blurrier outputs.
- L1 applied linear penalty → sharper contrast with small artifacts.
- Demonstrated implicit likelihood assumptions (Gaussian vs Laplace).

<Image src="/images/projects/Image_Generation/VAE_L2_L1.png" alt="L2 vs L1 Loss Comparison" title="Loss Difference" size="100" />


### 3.2 GAN Instability and Mode Collapse
- Rapid discriminator convergence led to **vanishing gradients**.
- Used **LeakyReLU**, reduced discriminator LR, and higher latent dimension to mitigate collapse.
- Found that **grayscale, low-texture datasets** like Fashion-MNIST reduce GAN feedback quality.
<Image src="/images/projects/Image_Generation/Gan_mode_collapse.png" alt="Mode Collapse Example" title="Mode Collapse" size="40" />


### 3.3 Diffusion Model Trade-offs
- Achieved best **FID and visual fidelity**, but required significantly more compute.
- Learned that **diffusion models unify VAEs’ coverage and GANs’ sharpness** at the cost of speed.

### 3.4 Metric Bias (FID Limitations)
- FID relies on Inception-V3 features → biased toward ImageNet textures.
- Assumes Gaussian feature distributions, simplifying true data manifolds.
- Despite bias, remains a reliable **ranking metric** for generative comparisons.

---

## **4. Outcome**

<TechTable leftHeader="Model" rightHeader="Summary">
  <TechRow label="VAE">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 59.1<br />
    Stable and diverse latent representations; edges soft and slightly blurred.
  </TechRow>

  <TechRow label="GAN">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 131.3<br />
    High contrast but repetitive samples due to partial mode collapse.
  </TechRow>

  <TechRow label="Diffusion (DDPM)">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 35.2<br />
    Crisp, detailed generations; excellent trade-off between fidelity and diversity.
  </TechRow>
</TechTable>

<Image src="/images/projects/Image_Generation/Outcomes.png" alt="Model Comparison Outcomes" title="FID Outcomes" size="100" />

--- 
**Key Takeaway:**  
- Diffusion models outperformed both VAEs and GANs, achieving **state-of-the-art realism and robustness** through iterative denoising.  
- VAEs provided **semantic stability** but limited sharpness; GANs produced **fine details** but suffered instability.  
- Diffusion achieved **best-in-class balance**, albeit with higher computational cost.

---

## **5. Conceptual Insights**

### **5.1 Variational Autoencoders (VAEs)**

VAEs learn a **smooth latent space** that tries to capture the global structure of the data distribution by optimizing a variational lower bound. They explicitly model \( p(x|z) \) and regularize \( q(z|x) \) toward a chosen prior (usually Gaussian).

#### 5.1.1 Strengths

- Learn **continuous, structured latent spaces** that are great for interpolation, clustering, and downstream tasks (few-shot KNN, simple classifiers, etc.).
- Training is **stable and predictable**: no adversarial game, just a single likelihood-style objective.
- Easy to extend with **conditional VAEs**, β-VAEs (disentanglement), or hierarchical latents.

#### 5.1.2 Weaknesses

- Tend to produce **blurry samples**, especially with simple decoders and L2 reconstruction loss, because they effectively average over uncertainty.
- The KL term can cause **posterior collapse** when the model ignores the latent code and relies on the decoder.
- Limited in capturing very sharp, fine-grained details compared to GANs or diffusion.

#### 5.1.3 Datasets where VAEs work best

- **Low- to medium-complexity, structured datasets** where global shape matters more than texture:  
  MNIST, Fashion-MNIST, simple grayscale symbols, medical scans with smooth intensity fields.
- Scenarios where the **latent space is as important as sample quality**: representation learning, anomaly detection, dimensionality reduction.

#### 5.1.4 Datasets where VAEs struggle

- **High-resolution natural images** with rich textures and sharp edges (e.g., ImageNet-level complexity) unless paired with very strong decoders.
- Domains where **tiny visual details** are critical (fine textures, crisp edges, photorealistic faces), as reconstructions tend to be over-smoothed.


### 5.2 **Generative Adversarial Networks (GANs)**

GANs learn to generate samples by playing a **minimax game** between a generator and a discriminator. They implicitly learn the data distribution by trying to fool the discriminator without ever computing an explicit likelihood.

#### 5.2.1 Strengths

- Can produce **extremely sharp, high-fidelity images** when training is successful.
- Good at capturing **fine local structure** and textures (edges, patterns, lighting).
- Sampling is very **fast at inference**: a single forward pass through the generator.

#### 5.2.2 Weaknesses

- Training is **unstable**: mode collapse, vanishing gradients, and sensitivity to architecture and hyperparameters.
- Do not give a true likelihood or an easy-to-use latent space; interpolation is possible but less principled than with VAEs.
- The discriminator can **overpower the generator** on simple or low-entropy datasets, leading to repeated patterns and low diversity.

#### 5.2.3 Datasets where GANs work best

- **Visually rich, high-variation datasets** where texture and sharpness matter: natural images, faces (CelebA), landscapes, art styles.
- Situations where **perceptual realism** is more important than strict coverage of all modes.

#### 5.2.4 Datasets where GANs struggle

- **Low-entropy or low-diversity datasets** (like grayscale digits/clothing) where the discriminator can quickly saturate and give poor gradient feedback, encouraging mode collapse (as seen on Fashion-MNIST in this project).
- Domains where **good mode coverage** and calibrated probabilities are important, not just visual realism.


### 5.3 **Diffusion Models (DDPMs)**

Diffusion models learn to reverse a **gradual noising process**: starting from pure Gaussian noise and iteratively denoising back to data space. They optimize a noise-prediction objective at many timesteps.

#### 5.3.1 Strengths

- Extremely **stable training**: the objective is supervised noise prediction, not adversarial.
- Achieve **state-of-the-art trade-off** between sample fidelity and diversity; they cover modes well while still producing crisp images.
- Naturally support **conditioning** (e.g., class labels, text, segmentation maps) by injecting information into the denoising network.

#### 5.3.2 Weaknesses

- **Slow sampling**: require many denoising steps at inference, although improved samplers (DDIM, distillation) can mitigate this.
- Computationally expensive during training due to repeated noise levels and large UNet backbones.
- More complex to implement and tune (noise schedules, timestep sampling, guidance, etc.) than basic VAE or GAN baselines.

#### 5.3.3 Datasets where Diffusion works best

- **Medium to high complexity image datasets** where both global structure and fine details matter: faces, objects, clothing, natural scenes.
- Tasks that benefit from **strong mode coverage and controllability**: conditional generation, text-to-image, inpainting, style transfer.
- Datasets with **consistent structure but varied details** (like Fashion-MNIST) where iterative refinement can sharpen silhouettes and textures.

#### 5.3.4 Datasets where Diffusion is less ideal

- Real-time or low-latency applications where **sampling cost is critical** (e.g., on-device generation with tight compute limits).
- Very simple datasets where the overhead of a full diffusion pipeline is overkill compared to a VAE or even a small GAN.


### 6. **Project Highlight**

Implemented and benchmarked three major generative architectures—VAE, GAN, and Diffusion—on Fashion-MNIST, achieving an FID of 35.2 with diffusion models. Explored the theoretical–practical trade-offs between reconstruction smoothness, adversarial sharpness, and iterative denoising stability, connecting them to modern Latent Diffusion Model research.

---