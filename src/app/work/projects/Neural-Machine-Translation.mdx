---
title: "Neural Machine Translation (NMT)"
publishedAt: "2024-10-11"
summary: "Implemented and benchmarked a full pipeline for Neural Machine Translation (NMT) using progressively advanced architectures."
images:
  - "/images/projects/NMT/Translation_Example.png"
  - "/images/projects/NMT/LSTM3-chain.png"
  - "/images/projects/NMT/LSTM3-SimpleRNN.png"
  - "/images/projects/NMT/attn.png"
  - "/images/projects/NMT/LSTM_Steps.png"
  - "/images/projects/NMT/Transformer.png"
  - "/images/projects/NMT/RNN-unrolled.png"
  - "/images/projects/NMT/mh_attn.png"
  - "/images/projects/NMT/attention.drawio.png"


team:
  - name: "Ketu Patel"
    role: "AI/ML Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/ketu-patel/"
---


## Overview

The project explored how machines learn to translate human language using deep sequence models. Beginning with recurrent architectures (RNNs, LSTMs), it progressed to attention-augmented Seq2Seq systems and ultimately to the Transformer, analyzing how architectural evolution impacts gradient flow, alignment, and translation coherence.

All models were trained on paired English captions and corresponding target translations using PyTorch implementations built from scratch, without relying on high-level libraries. Metrics such as loss and perplexity were tracked across training and validation to quantify language-model confidence and generalization.


## Table of Contents

- [1. Key Features](#1.-key-features)
  - [1.1 Recurrent Neural Network (RNN)](#1.1-recurrent-neural-network-rnn)
  - [1.2 Long Short-Term Memory (LSTM)](#1.2-long-short-term-memory-lstm)
  - [1.3 RNN with Attention](#1.3-rnn-with-attention)
  - [1.4 LSTM with Attention (Seq2Seq + Attention)](#1.4-lstm-with-attention-seq2seq-attention)
  - [1.5 Transformer](#1.5-transformer)
- [2. Technologies / Algorithms Used](#2.-technologies-algorithms-used)
- [3. Challenges & Learning](#3.-challenges-and-learning)
  - [3.1 Vanishing Gradients in RNNs](#3.1-vanishing-gradients-in-rnns)
  - [3.2 Context Vector Bottleneck](#3.2-context-vector-bottleneck)
  - [3.3 Stabilizing LSTM Training](#3.3-stabilizing-lstm-training)
  - [3.4 Attention Convergence and Alignment Failures](#3.4-attention-convergence-and-alignment-failures)
  - [3.5 Transformer Sensitivity to Optimization](#3.5-transformer-sensitivity-to-optimization)
  - [3.6 Using Attention to Understand Model Behavior](#3.6-using-attention-to-understand-model-behavior)
- [4. Outcomes / Results](#4.-outcomes-results)
  - [4.1 Seq2Seq Family (RNN → LSTM + Attention)](#4.1-seq2seq-family-rnn-lstm-attention)
  - [4.2 Transformer Family (Parallel Self-Attention Models)](#4.2-transformer-family-parallel-self-attention-models)
  - [4.3 Comparative Translation Outputs](#4.3-comparative-translation-outputs)
  - [4.4 Comparative Insights](#4.4-comparative-insights)
- [5. Conceptual Insights](#5.-conceptual-insights)
  - [5.1 RNN — Sequential Memory and Its Limits](#5.1-rnn-sequential-memory-and-its-limits)
  - [5.2 LSTM — Gated Long-Term Memory](#5.2-lstm-gated-long-term-memory)
  - [5.3 RNN + Attention — Breaking the Bottleneck](#5.3-rnn-attention-breaking-the-bottleneck)
  - [5.4 LSTM + Attention — Hybrid Strengths](#5.4-lstm-attention-hybrid-strengths)
  - [5.5 Transformer — Parallel Relational Reasoning](#5.5-transformer-parallel-relational-reasoning)
- [6. Project Highlights](#6.-project-highlights)

## **1. Key Features**

This section breaks down the five major model architectures implemented — each representing a distinct stage in the evolution of deep sequence modeling for translation.

### **1.1 Recurrent Neural Network (RNN)**

- The baseline model implemented a classical sequence-to-sequence (Seq2Seq) structure with separate encoder and decoder RNNs.
- Each encoder step processes one token at a time and updates a hidden state, which serves as a running summary of all previous tokens. After reading the entire input sequence, the final hidden state of the encoder is passed to the decoder as a condensed vector representation of the source sentence. The decoder then generates each target token autoregressively.
  
  <Image src="/images/projects/NMT/RNN-unrolled.png" alt="RNN-unrolled" title="RNN-Unrolled" size="70" />
  <Image src="/images/projects/NMT/LSTM3-SimpleRNN.png" alt="Simple-RNN" title="Simple-RNN" size="70" />



- This simple recurrent formulation forces the entire source sentence into a single vector representation. While computationally light, this approach suffers from the vanishing gradient problem, causing earlier words to be forgotten as new tokens are processed. This limitation makes it difficult for the model to retain long-range dependencies — particularly in long or complex sentences.
- In the translation pipeline, the encoder–decoder architecture was identical to the RNN model, but with LSTM cells enabling better temporal context and richer sequence representations. The LSTM provided the foundation for robust and stable optimization in the later attention and transformer experiments.

---

### **1.2 Long Short-Term Memory (LSTM)**

- To overcome the gradient vanishing problem of vanilla RNNs, the next model replaced the recurrent cell with a Long Short-Term Memory (LSTM) unit.
- The LSTM introduces three gating mechanisms — input, forget, and output gates — that regulate information flow and maintain a persistent cell state across time steps using forget gate, input gate,
cell state update, cell state, output gate, and hidden state:

<Image src="/images/projects/NMT/LSTM_Steps.png" alt="LSTM-operation" title="LSTM Operations" size="40" />
<Image src="/images/projects/NMT/LSTM3-chain.png" alt="LSTM" title="LSTM" size="70" />


- This gating mechanism acts like a controlled memory cell, preserving long-term dependencies and selectively forgetting irrelevant information. Unlike an RNN’s compressed hidden state, the LSTM’s dual-state structure allows gradient flow to remain nearly constant through time, dramatically improving convergence stability.
- In the translation pipeline, the encoder–decoder architecture was identical to the RNN model, but with LSTM cells enabling better temporal context and richer sequence representations. The LSTM provided the foundation for robust and stable optimization in the later attention and transformer experiments.

----

### **1.3 RNN with Attention**

- While LSTMs significantly improve memory retention, even they struggle to encode all sentence information into a single vector — a bottleneck known as the context vector limitation. To resolve this, an attention mechanism was introduced.
- In this variant, the decoder no longer relies on a single fixed-length vector. Instead, at each decoding step, it computes a similarity score between the current decoder state and all encoder hidden states. These scores are converted into a softmax distribution, which acts as a set of attention weights, denoting how much the decoder should focus on each input token.

<Image src="/images/projects/NMT/RNN_Attention.png" alt="RNN-Attention" title="RNN with Attention" size="70" />


- This dynamic alignment process enables the decoder to “look back” at relevant parts of the source sentence while generating each word. Conceptually, attention acts like a learned lookup table, assigning focus to semantically aligned words.
- This architecture marks a major step toward human-like translation, where the model attends selectively rather than relying on pure memorization.
----

<Image src="/images/projects/NMT/attention.drawio.png" alt="Attention" title="Attention-Mechanism" size="40" />

----
### **1.4 LSTM with Attention (Seq2Seq + Attention)**
- The next evolution combined the stability of LSTMs with the contextual flexibility of attention.
Here, both the encoder and decoder used LSTM units, and a cosine-similarity-based attention module was placed between them. The attention mechanism learned token-level dependencies across the entire source sequence, allowing the decoder to access any encoder output at any time.
- Architecturally, this system forms a complete neural translation pipeline with three interdependent modules:

  - **Encoder:** Maps input tokens to latent representations using bidirectional LSTMs.
  - **Attention Mechanism:** Computes token-level alignment via cosine similarity.
  - **Decoder:** Generates the target sentence step-by-step, combining embedding and attended context vectors.

- This combination retains long-term structure while dynamically attending to specific words, mimicking human translation where focus shifts fluidly between parts of the source sentence.

### **1.5 Transformer**
- The final and most advanced architecture implemented was the Transformer, a fully parallelized attention-based model that removes recurrence and convolutions altogether.
- Instead of processing one token at a time, the Transformer models global dependencies using self-attention. Each token attends to all other tokens in the sequence, learning contextual relationships in a single step.
- The key components include:
  
  - **Positional Encoding:** Adds sinusoidal position embeddings so the model retains sequence order in the absence of recurrence.
  - **Multi-Head Self-Attention:** Multiple attention heads operate in parallel, projecting input tokens into separate subspaces and attending to different contextual relationships simultaneously.
    - Queries, Keys, and Values for each token are computed via linear projections.
    - Attention weights are computed as:
      <Image src="/images/projects/NMT/attn.png" alt="Attention" title="Query, Key, and Value Weight Attention" size="50" />
      <Image src="/images/projects/NMT/mh_attn.png" alt="Multi-Head Attention" title="Multi-Head Attention" size="80" />


    
  - **Feed-Forward Network (FFN):** A two-layer MLP applied independently to each token, expanding and refining token-level representations.
  - **Residual Connections + Layer Normalization:** Stabilize gradients, allowing for much deeper architectures without loss explosion or gradient drift.
  - **Cross-Attention (Decoder):** The decoder includes both masked self-attention (so the model can’t look ahead) and encoder–decoder attention to align output tokens with relevant source tokens.

  <Image src="/images/projects/NMT/Transformer.png" alt="Transformer" title="Transformer Architecture" size="60" />

- Together, these innovations allow the Transformer to capture long-range dependencies globally rather than sequentially. Training becomes faster and more stable, while the model learns complex word alignments and syntactic structures naturally.


## **2. Technologies / Algorithms Used**
--- 

<TechTable leftHeader="Category" rightHeader="Details" alignDetails="left">

  <TechRow label="Frameworks & Libraries">
    - PyTorch 2.x, NumPy, tqdm, matplotlib 
  </TechRow>

  <TechRow label="Computing Environment">
    - Dockerized Env: Ubuntu 22.04 - Python 3.11 - CUDA 12.4 - cuDNN 8.9 <br />
    - GPU: NVIDIA RTX-series (16 GB) <br />
    - Development tools: VSCode, Jupyter Notebook, Anaconda <br />
    - Source control: GitHub integration <br />
  </TechRow>

  <TechRow label="Dataset">
    - German–English caption pairs from the Multi30k-style dataset
      - Preprocessed via tokenization
      - Vocabulary construction 
      - Padding 
      - Initialization and Termination (`<sos>/<eos>`) tokens.
  </TechRow>

  <TechRow label="Architectures">
    - Five core models implemented from scratch:  
      - RNN Encoder–Decoder  
      - LSTM Encoder–Decoder  
      - RNN + Attention (Bahdanau-style)  
      - LSTM + Attention (Seq2Seq) 
      - Full Transformer Encoder–Decoder
  </TechRow>

  <TechRow label="Recurrent Models (RNN / LSTM)">
    - The RNN propagated a single hidden state sequentially across tokens.  
    - The LSTM introduced gated cells (input, forget, output) and a persistent cell state for long-range gradient flow.  
    - Both encoder and decoder were implemented manually using `nn.Parameter` for gate weights and activations.
  </TechRow>

  <TechRow label="Seq2Seq Architecture + Attention Mechanism">
    - Comprised an Encoder (encodes source embeddings into latent vectors) and Decoder (generates tokens autoregressively).  
    - Training used teacher forcing; inference used greedy decoding.  
    - Implemented cosine-similarity attention between decoder queries and encoder outputs for token-level context alignment, computing relevance scores αₜ across encoder outputs per decoding step. 
  </TechRow>

  <TechRow label="Transformer Architecture">
    - Built a full Encoder–Decoder Transformer with:  
      - Embedding + Positional Encoding
      - Multi-Head Self-Attention (4 heads, hidden_dim=256)  
      - Feed-Forward Layer (dim_feedforward=3072, ReLU activation)  
      - Residual Connections and Layer Normalization after each sublayer  
      - Positional Encoding for sequence order retention  
      - Masked Self-Attention in the decoder + Cross-Attention linking encoder and decoder states.  
    - Validated against PyTorch’s `nn.Transformer` reference for correctness.
  </TechRow>

  <TechRow label="Optimization & Loss">
    - Optimized all models using <strong>Adam</strong> (β₁ = 0.9, β₂ = 0.999). 
    - Used Cross-Entropy Loss with <code>ignore_index=PAD_IDX</code> so padding tokens do not contribute to the loss. 
    - Managed learning rate with ReduceLROnPlateau schedulers that lower LR when training loss plateaus.
    - Applied gradient clipping and LR warmup for Transformer stability.
  </TechRow>

  <TechRow label="Regularization">
    - Applied Dropout (0.2–0.3) to embeddings and hidden layers in Seq2Seq models to mitigate overfitting from larger hidden/embedding dimensions. 
    - In Transformers, used Dropout within attention and FFN blocks plus LayerNorm to stabilize training with residual connections.
  </TechRow>

  <TechRow label="Training Configuration">
    - Seq2Seq runs: 20–40 epochs, batch size 128–256, embedding/hidden dimensions 128–256 
    - Transformer runs: 10–16 epochs, hidden_dim=256, 4 heads, FFN dim=2048–3072. 
    - Teacher forcing ratio set to 1.0 for training; greedy decoding used at inference.
  </TechRow>

  <TechRow label="Evaluation Metrics">
    - Perplexity (PPL)— computed as <code>exp(average_loss)</code>, measuring next-token uncertainty. 
    - Supplementary: loss curves, translation fluency, and qualitative alignment analysis.

  </TechRow>

  <TechRow label="Visualization & Analysis">
    - Generated **training/validation loss and perplexity curves** for all models.  
    - Visualized **attention heatmaps** to inspect source–target alignment quality.  
    - Compared **cosine-similarity distributions** and cross-attention patterns across architectures.
  </TechRow>

  <TechRow label="Implementation Notes">
    - All networks implemented — including gate-level RNNs, LSTM units, attention scoring, and Transformer blocks.  
    - Configurations and checkpoints logged via **YAML-based system** for reproducibility.
  </TechRow>

</TechTable>

---

## **3. Challenges & Learning**
  ### **3.1 Vanishing Gradients in RNNs**    
    Early experiments with the vanilla RNN Seq2Seq architecture revealed the classical vanishing-gradient failure mode: as sequence length increased, gradients propagated through many recurrent steps and diminished rapidly. This resulted in loss of information from the first half of sentences, producing translations that captured the end of the sentence but often ignored the beginning. Training curves reflected this: loss dropped slowly and plateaued early on longer examples. This reinforced a central limitation of simple RNNs—their compressed hidden representation and nonlinear recurrence make long-term dependency retention inherently unstable.
  
  ### **3.2 Context Vector Bottleneck** 
    Even after replacing RNN cells with LSTMs to stabilize gradient flow, the canonical Seq2Seq design still relied on a single fixed-length context vector to represent the entire source sentence. The Multi30k German→English dataset contains descriptive captions that frequently involve long noun phrases and compositional details; compressing such content into one vector systematically caused semantic omissions and structural distortions. The model often captured the gist of the caption but dropped attributes like color, location, or modifiers. This exposed the structural flaw of the “encoder bottleneck” design and motivated the introduction of attention.

  ### **3.3 Stabilizing LSTM Training**
    LSTMs provided dramatic improvement, but their high capacity made them sensitive to hyperparameters. Larger hidden dimensions initially led to overfitting, where training perplexity dropped rapidly while validation perplexity stagnated. Adding dropout, tuning learning rates, and moderating hidden sizes produced significantly more stable convergence. This revealed a practical lesson: LSTMs excel on small datasets only when regularization and capacity are balanced carefully, especially with descriptive caption data like Multi30k.

  ### **3.4 Attention Convergence and Alignment Failures**
    Implementing attention introduced its own challenges. Early versions produced flat, noisy, or diffuse alignment weights, with the decoder showing no clear preference for relevant encoder tokens. As a result, translations were fluently structured but conceptually mismatched. Switching to cosine similarity normalization, proper softmax scaling, and weight initialization fixed these issues. Attention began to form diagonal alignment patterns, especially for monotonic caption structures. This showed that attention is powerful but numerically fragile, requiring stable geometry and scoring functions.

  ### **3.5 Transformer Sensitivity to Optimization**
    The Transformer brought superior capacity but was much more sensitive to optimization choices. High learning rates or insufficient warmup caused loss oscillations, early divergence, or failure to learn meaningful attention patterns. Reducing LR, extending training duration, and correctly applying dropout and LayerNorm produced smooth convergence. These challenges emphasized the trade-off: Transformers achieve the best results, but demand careful, theory-aligned optimization strategies.
    
  ### **3.6 Using Attention to Understand Model Behavior**
    Attention heatmaps across models revealed stark architectural differences. Seq2Seq attention showed mostly monotonic alignments—suitable for caption-style datasets. Transformers displayed multi-head specialization, with heads attending to nouns, verbs, and spatial descriptors differently. This interpretability confirmed that improved perplexities were not accidental: models were learning linguistic structure, not just memorizing patterns. The process underscored how visualization helps validate whether models are improving for the right reasons.

---

## **4. Outcomes / Results**

### **4.1 Seq2Seq Family (RNN → LSTM + Attention)**

<TechTable columns={["Model","Train Loss","Val Loss","Train PPL","Val PPL","Summary / Insight"]}>
  <TechRow label="RNN (Baseline)" values={["4.61","4.70","100.6","110.0","Struggled to preserve early context. Gradients vanished through long sequences, causing the model to focus only on the last few tokens and produce truncated translations."]} />
  <TechRow label="LSTM (No Attention)" values={["3.23","3.39","25.4","29.7","Gated memory cells maintained long-term gradients, yielding more stable training and improved fluency for shorter sentences."]} />
  <TechRow label="RNN + Attention" values={["3.20","3.42","26.8","30.6","Attention introduced dynamic alignment, allowing selective focus across source tokens. Improved context usage but still limited by shallow recurrence."]} />
  <TechRow label="LSTM + Attention (Best Seq2Seq)" values={["2.54","3.13","12.6","22.9","Combined the LSTM’s memory stability with token-level attention. Achieved sharp alignments, coherent syntax, and strong semantic coverage."]} />
</TechTable>

---


### **4.2 Transformer Family (Parallel Self-Attention Models)**

<TechTable columns={["Model","Train Loss","Val Loss","Train PPL","Val PPL","Summary / Insight"]}>
  <TechRow label="Encoder-Only Transformer" values={["–","–","8.6","22.5","Served as a partial baseline without cross-attention. Produced monotonic, less context-aware translations but demonstrated strong sentence embeddings."]} />
  <TechRow label="Full Transformer (Default Hyperparameters)" values={["1.49","1.83","4.46","6.24","Multi-head self-attention captured global dependencies and improved fluency. Parallelism accelerated training but required careful learning-rate tuning."]} />
  <TechRow label="Full Transformer (Tuned Hyperparameters)" values={["1.20","1.79","3.33","6.02","Best-performing model. Tuned dropout, feed-forward width, and learning rate to achieve smooth convergence, low uncertainty, and high-quality translations."]} />
</TechTable>

---


### **4.3 Comparative Translation Outputs**

<TechTable columns={["Sentence", "German Tokenized Input", "True Translation", "LSTM with Attention", "Transformer (Encoder Only)", "Full Transformer"]} style={{ maxWidth: "1800px", width: "100%", overflowX: "auto", margin: "0 auto" }}>
  <TechRow label="1" values={["['<sos>', 'ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', 'der', 'etwas', '<unk>', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'man', 'wearing', 'a', 'orange', 'hat', 'is', 'to', 'something', '<eos>’, ..]", "['<sos>', 'a', 'man', 'in', 'an', 'hat', 'hat', 'hat', 'something', 'something', 'something', '<eos>’, ..]", "['<sos>', 'a', 'man', 'in', 'an', 'orange', 'hat', '<unk>', 'something', '<unk>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="2" values={["['<sos>', 'ein', 'boston', 'terrier', 'läuft', 'über', '<unk>', 'grünes', 'gras', 'vor', 'einem', 'weißen', 'zaun', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'boston', 'terrier', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '<eos>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'white', 'team', 'is', 'running', 'through', 'the',  'white', 'grass', 'of', 'of', 'of', 'a', '<eos>’, ..]", "['<sos>', 'a', 'boston', 'vendor', 'runs', 'running', 'grass', 'green', 'the', 'grass', 'of', 'of', 'of', 'fence', 'fence', 'fence', '<eos>’, ..]", "['<sos>', 'a', 'boston', '<unk>', 'dog', 'is', 'running', 'across', 'a', 'white', 'fence', 'in', 'front', 'of', 'a', 'white', 'fence', '<eos>', 'grass', '<eos>']"]} />
  <TechRow label="3" values={["['<sos>', 'ein', 'mädchen', 'in', 'einem', 'karateanzug', 'bricht', 'ein', 'brett', 'mit', 'einem', 'tritt', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'girl', 'in', 'karate', 'uniform', 'breaking', 'a', 'stick', 'with', 'a', 'front', 'kick', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'girl', 'in', 'a', 'a', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>’, ..]", "['<sos>', 'a', 'girl', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>’,]", "['<sos>', 'a', 'girl', 'in', 'a', 'drill', 'is', 'putting', 'a', 'board', 'with', 'a', 'video', 'camera', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="4" values={["['<sos>', 'fünf', 'leute', 'in', 'winterjacken', 'und', 'mit', 'helmen', 'stehen', 'im', 'schnee', 'mit', '<unk>', 'im', 'hintergrund', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'five', 'people', 'wearing', 'winter', 'jackets', 'and', 'helmets', 'stand', 'in', 'the', 'snow', 'with', '<unk>', 'in', 'the', 'background', '<eos>', '<pad>', '<pad>']", "['<sos>', 'five', 'people', 'in', 'in', 'and', 'and', 'and', 'and', 'with', 'in', 'in', 'in', 'background', 'background',  '<eos>’, ..]", "['<sos>', 'five', 'people', 'in', 'in', 'and', 'helmets', 'helmets', 'helmets', 'helmets', 'in', 'snow', 'in', 'the', 'the', 'in', '<eos>', 'background', 'background', '<eos>']", "['<sos>', 'five', 'people', 'in', 'bikinis', 'and', 'helmets', 'stand', 'in', 'the', 'snow', 'with', 'helmets', 'in', 'the', 'background', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="5" values={["['<sos>', 'leute', 'reparieren', 'das', 'dach', 'eines', 'hauses', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'people', 'are', 'fixing', 'the', 'roof', 'of', 'a', 'house', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'people', 'on', 'the', 'the', 'of', 'a', 'of', '<eos>’,..]", "['<sos>', 'people', 'are', 'the','the', 'a', 'a', '<eos>’, ..]", "['<sos>', 'people', 'waving', 'the', 'roof', 'of', 'a', 'house', 'on', 'a', 'house', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="6" values={["['<sos>', 'eine', 'gruppe', 'von', 'menschen', 'steht', 'vor', 'einem', 'iglu', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'group', 'of', 'people', 'standing', 'in', 'front', 'of', 'an', 'igloo', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'group', 'of', 'people', 'standing', 'standing', 'front', 'front', 'a', '<eos>’, ..]", "['<sos>', 'a', 'group', 'of', 'people', 'standing', 'standing', 'in', 'a', '<eos>’,..]", "['<sos>', 'a', 'group', 'of', 'people', 'stand', 'in', 'front', 'of', 'a', 'fire', 'pit', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="7" values={["['<sos>', 'ein', 'typ', 'arbeitet', 'an', 'einem', 'gebäude', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'guy', 'works', 'on', 'a', 'building', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'guy', 'is', 'working', 'on', 'a', 'building', 'building', '<eos>’, ..]", "['<sos>', 'a', 'guy', 'is', 'on', 'a', 'building', 'building', '<eos>’, ..]", "['<sos>', 'a', 'guy', 'working', 'on', 'a', 'building', 'with', 'a', 'building', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="8" values={["['<sos>', 'ein', 'mann', 'in', 'einer', 'weste', 'sitzt', 'auf', 'einem', 'stuhl', 'und', 'hält', 'magazine', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'man', 'in', 'a', 'vest', 'is', 'sitting', 'in', 'a', 'chair', 'and', 'holding', 'magazines', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'man', 'in', 'a', 'suit', 'is', 'sitting', 'a', 'a', 'and', 'a', '<eos>’, ..]", "['<sos>', 'a', 'man', 'in', 'a', 'vest', 'vest', 'on', 'on', 'a',  'holding', 'a', 'and', '<eos>’, ..]", "['<sos>', 'a', 'man', 'in', 'a', 'vest', 'is', 'sitting', 'on', 'a', 'chair', 'holding', 'umbrellas', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']"]} />
  <TechRow label="9" values={["['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']", "['<sos>', 'a', 'mother', 'and', 'her', 'her', 'her', 'her', 'on', 'on', 'the', 'day', '<eos>’, ..]", "['<sos>', 'a', 'mother', 'and', 'her', 'small', 'son', 'enjoying', 'a','<eos>', 'day', 'day', '<eos>’, ..]", "['<sos>', 'a', 'mother', 'and', 'little', 'girl', 'enjoy', 'a', 'nice', 'day', 'outdoors', 'outside', '<eos>', '<eos>', 'day', '<eos>', '<eos>', 'day', '<eos>', '<eos>']"]} />
</TechTable>
---

### **4.4 Comparative Insights**

<TechTable leftHeader="Aspect" rightHeader="Observation" alignDetails="left">

  <TechRow label="Performance Evolution Across Architectures">
    - Every architectural upgrade brought a clear reduction in perplexity and more coherent translation outputs. 
    - Moving from RNN → LSTM reduced perplexity by ~4×, thanks to stable gradient paths. 
    - Adding attention removed the context bottleneck and improved semantic accuracy. 
    - Transformers delivered another major improvement—roughly 3× lower perplexity than Seq2Seq—reflecting their ability to model long-range, non-monotonic dependencies.
  </TechRow>

  <TechRow label="Strength of Architectural Innovations">
    - LSTM gating preserved long-term information and mitigated gradient decay.
    - Attention mechanisms eliminated the need to compress all meaning into a single vector, enabling token-level alignment.
    - Transformer self-attention offered full parallel context modeling, capturing global relationships and reordering patterns far better than recurrent models.
  </TechRow>

  <TechRow label="Dataset Suitability">
    The Multi30k-style caption dataset contains short-to-medium-length, mostly monotonic, descriptive sentences—a regime where:
      - RNNs struggle due to long-range dependencies.
      - LSTMs perform well but plateau on complex captions.
      - Seq2Seq with attention performs strongly due to clear alignment structure.
    Transformers excel due to global relational modeling and ability to integrate spatial/visual descriptors.

Transformers are somewhat “overpowered” for this dataset, yet they still produce the highest-quality translations and lowest perplexities.
  </TechRow>

  <TechRow label="Qualitative Behavior">
    Example comparisons showed:
      - RNNs lost early nouns and attributes (“a man … something”).
      - LSTMs captured sentence structure but sometimes omitted modifiers.
      - Attentional Seq2Seq aligned source and target tokens correctly, preserving detail.
      - Transformers produced the most fluent, globally coherent translations—even when restructuring sentence order.

    Concrete example
    - Ground truth: “A girl in karate uniform breaking a stick with a front kick.”
    - LSTM + Attention: “A girl in a uniform kicking something.”
    - Transformer: “A girl in a karate uniform is kicking a board with a ball.”
    The Transformer maintained subject–verb agreement, captured object relations, and produced fluent natural sentences closer to human translation quality.

  </TechRow>

</TechTable>

---

## **5. Conceptual Insights**

### **5.1 RNN — Sequential Memory and Its Limits**

  #### **5.1.1 Strengths**
    - Simple structure and easy to implement from scratch.
    - Suitable for very short, local-dependency tasks.
    - Low memory footprint compared to LSTMs and Transformers.

  #### **5.1.2 Weaknesses**
    - Suffers severely from vanishing gradients.
    - Cannot capture long-range dependencies due to hidden state compression.
    - One hidden state must encode the entire meaning → major bottleneck.
    - Training becomes unstable as sentence length increases.

  #### **5.1.3 Dataset Suitability (Multi30k Captions)**
    - Mostly unsuitable.
    - The caption dataset includes descriptors (colors, objects, spatial info) that RNNs frequently lose.
    - Works only on extremely short captions with minimal complexity.

### **5.2 LSTM — Gated Long-Term Memory**

  #### **5.2.1 Strengths**
    - Input, output, and forget gates stabilize gradient flow.
    - Captures longer dependencies far more reliably than vanilla RNNs.
    - Dual-state structure (hidden + cell state) improves representational richness.
    - Better at retaining early sentence structure.

  #### **5.2.2 Weaknesses**
    - Large number of parameters → can overfit small datasets.
    - Training is slower than RNNs due to gate computations.
    - Still compresses entire meaning into a single final state (bottleneck persists).
    - Sequential decoding → cannot parallelize token generation.

  #### **5.2.3 Dataset Suitability (Multi30k Captions)**
    - Good for Multi30k captions.
    - Can handle typical 8–14 word captions with modest complexity.
    - Still loses details in longer or multi-clause captions without attention.

### **5.3 RNN + Attention — Breaking the Bottleneck**

  #### **5.3.1 Strengths**
    - Eliminates the fixed-length context vector → allows flexible retrieval of information.
    - Attention scores act like soft alignments between tokens.
    - Supports more complete translations with fewer omissions.
    - Heatmaps provide excellent interpretability of alignment patterns.

  #### **5.3.2 Weaknesses**
    - Still bottlenecked by RNN recurrence.
    - Slow decoding and training compared to Transformers.
    - Sensitive to attention scoring stability.
    - Struggles when alignment is non-monotonic.

  #### **5.3.3 Dataset Suitability (Multi30k Captions)**
    - Very strong.
    - Caption translation is mostly monotonic—attention heatmaps form near-diagonal patterns.
    - Great for preserving object–attribute relationships (e.g., “orange hat,” “green grass”).

### **5.4 LSTM + Attention — Hybrid Strengths**

  #### **5.4.1 Strengths**
    - Combines LSTM’s memory stability with attention’s contextual flexibility.
    - Produces the best results among Seq2Seq models.
    - Handles long noun phrases and descriptive attributes effectively.
    - Highly interpretable and visually aligned with human translation flow.

  #### **5.4.2 Weaknesses**
    - Still slow due to sequential decoding.
    - Requires careful regularization on small datasets to avoid overfitting.
    - Attention maps occasionally noisy for rare words.

  #### **5.4.3 Dataset Suitability (Multi30k Captions)**
    - Excellent.
    - Multi30k contains descriptive captions—this architecture effectively captures spatial and visual details.
    - Preserves sentence structure without the over-parameterization risk of Transformers.

### **5.5 Transformer — Parallel Relational Reasoning**

  #### **5.5.1 Strengths**
    - Fully parallelized → fastest training on GPUs.
    - Multi-head self-attention models many linguistic relationships simultaneously.
    - No recurrence → no long-range gradient decay.
    - Best global coherence, fluency, and reordering ability.
    - Residual connections + LayerNorm support deep architectures.

  #### **5.5.2 Weaknesses**
    - High memory usage.
    - Requires precise LR, warmup, dropout, and normalization for stability.
    - Overfits easily when dataset is small without proper regularization.
    - Multi-head attention can collapse without tuning.

  #### **5.5.3 Dataset Suitability (Multi30k Captions)**
    - Very good to excellent.
    - While Multi30k is small for a Transformer, moderate dropout and tuning prevent overfitting.
    - Captures global sentence structure and subtle conceptual relations (e.g., subject–verb–object alignment).

---

## **6. Project Highlights**

- Implemented five neural machine translation architectures from scratch (RNN, LSTM, RNN+Attention, LSTM+Attention, Transformer), demonstrating the full historical progression of sequence modeling.
- Designed a fully custom NMT pipeline, including:
  - Vocabulary Construction
  - Tokenization and Padding
  - Batching
  - Perplexity tracking and LR scheduling
- Performed rigorous hyperparameter tuning, including adjustments to:
  - Hidden sizes
  - FFN widths
  - Attention Head count
  - Dropout and LR schedules
  - Teacher Forcing and Gradient Clipping
- Generated comprehensive evaluation artifacts, including:
  - Perplexity and Loss Curves
  - Side-by-side Translation Comparisons (including spreadsheet outputs)
  - Qualitative Alignment Analysis

- Connected empirical findings to modern NLP theory, showing:
  - Why LSTMs outperform RNNs
  - How attention overcomes the context bottleneck
  - Why Transformers achieve the best alignment and fluency
- Demonstrated clear performance improvements, culminating in the tuned Transformer model achieving the lowest perplexity and highest translation coherence.

---