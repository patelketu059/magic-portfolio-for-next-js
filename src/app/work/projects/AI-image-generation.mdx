---
title: "AI Image Generation"
publishedAt: "2025-11-06"
summary: "Using Generative AI alogrithms including VAEs, GANs and Diffusion models."
images:
  - "/images/projects/Image_Generation/Outcomes.png"
  - "/images/projects/Image_Generation/Gan_mode_collapse.png"
  - "/images/projects/Image_Generation/Forward_diff_epoch.png"
  - "/images/projects/Image_Generation/Reverse_diff_epoch.png"
  - "/images/projects/Image_Generation/Gan_Objective.png"
  

team:
  - name: "Ketu Patel"
    role: "AI/ML Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/ketu-patel/"
---

## Overview


This project explored modern generative modeling techniques—Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Denoising Diffusion Probabilistic Models (DDPMs)—to learn and synthesize image distributions from MNIST and Fashion-MNIST datasets.

The study implemented and analyzed each model’s architecture, training dynamics, and output quality, benchmarking them using the Frechet Inception Distance (FID) metric to quantify realism and diversity.
A research review of Latent Diffusion Models (LDMs) complemented the practical component, connecting theoretical insights with experimental performance.

### Table of Contents

- [1. Key Features](#1.-key-features)
  - [1.1 Variational Autoencoder (VAE)](#1.1-variational-autoencoder-vae)
  - [1.2 Generative Adversarial Network (GAN)](#1.2-generative-adversarial-network-gan)
  - [1.3 Diffusion Models (DDPM)](#1.3-diffusion-models-ddpm)
- [2. Technologies / Algorithms Used](#2.-technologies-algorithms-used)
- [3. Challenges & Learning](#3.-challenges-learning)
  - [3.1 Balancing Reconstruction vs Regularization (VAEs)](#3.1-balancing-reconstruction-vs-regularization-vaes)
  - [3.2 GAN Instability and Mode Collapse](#3.2-gan-instability-and-mode-collapse)
  - [3.3 Diffusion Model Trade-offs](#3.3-diffusion-model-trade-offs)
  - [3.4 Metric Bias (FID Limitations)](#3.4-metric-bias-fid-limitations)
- [4. Outcome](#4.-outcome)
- [5. Conceptual Insights](#5.-conceptual-insights)
  - [5.1 Variational Autoencoders (VAEs)](#5.1-variational-autoencoders-vaes)
  - [5.2 Generative Adversarial Networks (GANs)](#5.2-generative-adversarial-networks-gans)
  - [5.3 Diffusion Models (DDPMs)](#5.3-diffusion-models-ddpms)
- [6. Project Highlight](#6.-project-highlight)
## **1. Key Features**

### **1.1 Variational Autoencoder (VAE)**

- Implemented the full probabilistic encoder–decoder pipeline:
  - **Reparameterization trick**:  
    `z = μ + σ * ε` to enable backpropagation through stochastic sampling.
  - **VAE loss** combining **reconstruction loss (L1/L2)** and **KL-divergence regularization**:

    <Image src="/images/projects/Image_Generation/L_KL.png" alt="KL Divergence Loss" title="KL Loss" size={40} />
    <Image src="/images/projects/Image_Generation/L_total.png" alt="Total Loss" title="Total Loss" size={40} />


  - Tuned **β** values to balance latent regularization and reconstruction fidelity.

- **Loss Function Comparison (L1 vs L2):**
  - L2 yielded smoother, blurrier digits (Gaussian-assumed error).
  - L1 preserved sharper edges and higher contrast (Laplace-assumed error).

- **Few-Shot Evaluation:**
  - Used encoder embeddings to train a **KNN classifier** on limited MNIST samples, testing latent generalization.

---

### **1.2 Generative Adversarial Network (GAN)**

- Built a full adversarial pipeline:
  - **Generator:** Maps random latent vectors → 28×28 images.
  - **Discriminator:** Outputs probability of real vs fake samples.
  - **Objective Function:**
  <Image src="/images/projects/Image_Generation/Gan_Objective.png" alt="GAN Objective" title="GAN Objective" size="70" />


- Implemented individual loss functions for real, fake, and generator batches.

- Investigated **mode collapse** behavior:
  - Varying learning rate, latent dimension, and activation (LeakyReLU).
  - Observed generator convergence on limited modes with high discriminator capacity.

----

### **1.3 Diffusion Models (DDPM)**

- Developed an end-to-end **Denoising Diffusion Probabilistic Model** following *Ho et al., 2020*.
  - **Forward Process:** Gradually corrupt images with Gaussian noise:
    <Image src="/images/projects/Image_Generation/Forward_diffusion.png" alt="Forward Diffusion" title="Forward Diffusion Formula" size="40" />
    <Image src="/images/projects/Image_Generation/Forward_diff_epoch.png" alt="Forward Demo" title="Forward Diffusion Epoch" size="95" />


  - **Reverse Process:** Train a neural network `ε_θ(x_t, t)` (UNet) to predict noise at each timestep and iteratively denoise to reconstruct `x_0`.
    <Image src="/images/projects/Image_Generation/Reverse_diffusion.png" alt="Reverse Diffusion" title="Reverse Diffusion Formula" size="40" />
    <Image src="/images/projects/Image_Generation/Reverse_diff_epoch.png" alt="Reverse Demo" title="Reverse Diffusion Epoch" size="95" />

  - Implemented a **linear noise schedule** and cached α_t and ᾱ_t for training efficiency.

- Visualized both **forward diffusion** (progressive noising) and **reverse diffusion** (iterative denoising) to interpret model behavior.

----

## **2. Technologies / Algorithms Used**
--- 

<TechTable leftHeader="Category" rightHeader="Details" alignDetails="left">
  <TechRow label="Frameworks">
    PyTorch 2.x, TorchVision, NumPy, Matplotlib, PyYAML
  </TechRow>

  <TechRow label="Datasets">
    MNIST and Fashion-MNIST (28×28 grayscale, 10 classes)
  </TechRow>

  <TechRow label="Architectures">
    VAE (Encoder–Decoder MLP), GAN (Generator + Discriminator), DDPM (UNet)
  </TechRow>

  <TechRow label="Optimization">
    AdamW optimizer, weight decay tuning, learning-rate scheduling
  </TechRow>

  <TechRow label="Loss Functions">
    L1, L2, KL-Divergence, Binary Cross-Entropy
  </TechRow>

  <TechRow label="Noise Scheduling (Diffusion)">
    Linear variance schedule βₜ ∈ [0.0001, 0.02]
  </TechRow>

  <TechRow label="Evaluation Metric">
    Frechet Inception Distance (FID) for realism and diversity
  </TechRow>

  <TechRow label="Config System">
    YAML-driven experiment configs with tracked outputs
  </TechRow>

  <TechRow label="Environment">
    Python 3.11, GPU-accelerated PyTorch, Dockerized setup
  </TechRow>

  <TechRow label="Optimization">
    AdamW optimizer, weight decay tuning, learning-rate scheduling
  </TechRow>

</TechTable>

---

## **3. Challenges & Learning**

### 3.1 Balancing Reconstruction vs Regularization (VAEs)
- High β values improved disentanglement but caused **posterior collapse**.
- Low β favored reconstruction detail but weakened latent structure.
- Learned to **tune β adaptively** by dataset complexity.

#### 3.1.1 Loss Dynamics (L1 vs L2)
- L2 minimized squared error → smoother but blurrier outputs.
- L1 applied linear penalty → sharper contrast with small artifacts.
- Demonstrated implicit likelihood assumptions (Gaussian vs Laplace).

<Image src="/images/projects/Image_Generation/VAE_L2_L1.png" alt="L2 vs L1 Loss Comparison" title="Loss Difference" size="100" />


### 3.2 GAN Instability and Mode Collapse
- Rapid discriminator convergence led to **vanishing gradients**.
- Used **LeakyReLU**, reduced discriminator LR, and higher latent dimension to mitigate collapse.
- Found that **grayscale, low-texture datasets** like Fashion-MNIST reduce GAN feedback quality.
<Image src="/images/projects/Image_Generation/Gan_mode_collapse.png" alt="Mode Collapse Example" title="Mode Collapse" size="40" />


### 3.3 Diffusion Model Trade-offs
- Achieved best **FID and visual fidelity**, but required significantly more compute.
- Learned that **diffusion models unify VAEs’ coverage and GANs’ sharpness** at the cost of speed.

### 3.4 Metric Bias (FID Limitations)
- FID relies on Inception-V3 features → biased toward ImageNet textures.
- Assumes Gaussian feature distributions, simplifying true data manifolds.
- Despite bias, remains a reliable **ranking metric** for generative comparisons.

---

## **4. Outcome**

<TechTable leftHeader="Model" rightHeader="Summary">
  <TechRow label="VAE">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 59.1<br />
    Stable and diverse latent representations; edges soft and slightly blurred.
  </TechRow>

  <TechRow label="GAN">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 131.3<br />
    High contrast but repetitive samples due to partial mode collapse.
  </TechRow>

  <TechRow label="Diffusion (DDPM)">
    <strong>Dataset:</strong> Fashion-MNIST<br />
    <strong>FID:</strong> 35.2<br />
    Crisp, detailed generations; excellent trade-off between fidelity and diversity.
  </TechRow>
</TechTable>

<Image src="/images/projects/Image_Generation/Outcomes.png" alt="Model Comparison Outcomes" title="FID Outcomes" size="100" />

--- 
**Key Takeaway:**  
- Diffusion models outperformed both VAEs and GANs, achieving **state-of-the-art realism and robustness** through iterative denoising.  
- VAEs provided **semantic stability** but limited sharpness; GANs produced **fine details** but suffered instability.  
- Diffusion achieved **best-in-class balance**, albeit with higher computational cost.

---

## **5. Conceptual Insights**

### **5.1 Variational Autoencoders (VAEs)**

VAEs learn a **smooth latent space** that tries to capture the global structure of the data distribution by optimizing a variational lower bound. They explicitly model \( p(x|z) \) and regularize \( q(z|x) \) toward a chosen prior (usually Gaussian).

#### 5.1.1 Strengths

- Learn **continuous, structured latent spaces** that are great for interpolation, clustering, and downstream tasks (few-shot KNN, simple classifiers, etc.).
- Training is **stable and predictable**: no adversarial game, just a single likelihood-style objective.
- Easy to extend with **conditional VAEs**, β-VAEs (disentanglement), or hierarchical latents.

#### 5.1.2 Weaknesses

- Tend to produce **blurry samples**, especially with simple decoders and L2 reconstruction loss, because they effectively average over uncertainty.
- The KL term can cause **posterior collapse** when the model ignores the latent code and relies on the decoder.
- Limited in capturing very sharp, fine-grained details compared to GANs or diffusion.

#### 5.1.3 Datasets where VAEs work best

- **Low- to medium-complexity, structured datasets** where global shape matters more than texture:  
  MNIST, Fashion-MNIST, simple grayscale symbols, medical scans with smooth intensity fields.
- Scenarios where the **latent space is as important as sample quality**: representation learning, anomaly detection, dimensionality reduction.

#### 5.1.4 Datasets where VAEs struggle

- **High-resolution natural images** with rich textures and sharp edges (e.g., ImageNet-level complexity) unless paired with very strong decoders.
- Domains where **tiny visual details** are critical (fine textures, crisp edges, photorealistic faces), as reconstructions tend to be over-smoothed.


### 5.2 **Generative Adversarial Networks (GANs)**

GANs learn to generate samples by playing a **minimax game** between a generator and a discriminator. They implicitly learn the data distribution by trying to fool the discriminator without ever computing an explicit likelihood.

#### 5.2.1 Strengths

- Can produce **extremely sharp, high-fidelity images** when training is successful.
- Good at capturing **fine local structure** and textures (edges, patterns, lighting).
- Sampling is very **fast at inference**: a single forward pass through the generator.

#### 5.2.2 Weaknesses

- Training is **unstable**: mode collapse, vanishing gradients, and sensitivity to architecture and hyperparameters.
- Do not give a true likelihood or an easy-to-use latent space; interpolation is possible but less principled than with VAEs.
- The discriminator can **overpower the generator** on simple or low-entropy datasets, leading to repeated patterns and low diversity.

#### 5.2.3 Datasets where GANs work best

- **Visually rich, high-variation datasets** where texture and sharpness matter: natural images, faces (CelebA), landscapes, art styles.
- Situations where **perceptual realism** is more important than strict coverage of all modes.

#### 5.2.4 Datasets where GANs struggle

- **Low-entropy or low-diversity datasets** (like grayscale digits/clothing) where the discriminator can quickly saturate and give poor gradient feedback, encouraging mode collapse (as seen on Fashion-MNIST in this project).
- Domains where **good mode coverage** and calibrated probabilities are important, not just visual realism.


### 5.3 **Diffusion Models (DDPMs)**

Diffusion models learn to reverse a **gradual noising process**: starting from pure Gaussian noise and iteratively denoising back to data space. They optimize a noise-prediction objective at many timesteps.

#### 5.3.1 Strengths

- Extremely **stable training**: the objective is supervised noise prediction, not adversarial.
- Achieve **state-of-the-art trade-off** between sample fidelity and diversity; they cover modes well while still producing crisp images.
- Naturally support **conditioning** (e.g., class labels, text, segmentation maps) by injecting information into the denoising network.

#### 5.3.2 Weaknesses

- **Slow sampling**: require many denoising steps at inference, although improved samplers (DDIM, distillation) can mitigate this.
- Computationally expensive during training due to repeated noise levels and large UNet backbones.
- More complex to implement and tune (noise schedules, timestep sampling, guidance, etc.) than basic VAE or GAN baselines.

#### 5.3.3 Datasets where Diffusion works best

- **Medium to high complexity image datasets** where both global structure and fine details matter: faces, objects, clothing, natural scenes.
- Tasks that benefit from **strong mode coverage and controllability**: conditional generation, text-to-image, inpainting, style transfer.
- Datasets with **consistent structure but varied details** (like Fashion-MNIST) where iterative refinement can sharpen silhouettes and textures.

#### 5.3.4 Datasets where Diffusion is less ideal

- Real-time or low-latency applications where **sampling cost is critical** (e.g., on-device generation with tight compute limits).
- Very simple datasets where the overhead of a full diffusion pipeline is overkill compared to a VAE or even a small GAN.


### 6. **Project Highlight**

Implemented and benchmarked three major generative architectures—VAE, GAN, and Diffusion—on Fashion-MNIST, achieving an FID of 35.2 with diffusion models. Explored the theoretical–practical trade-offs between reconstruction smoothness, adversarial sharpness, and iterative denoising stability, connecting them to modern Latent Diffusion Model research.

---